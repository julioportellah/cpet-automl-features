{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to use Azure ML 1.30.0 to work with jp-ml-workspace-train\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1624365619471
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpet_data = ws.datasets.get(\"cpet dataset cardiac2\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624365625657
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "experiment_folder = 'cpet_training-hyperdrive'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print('Folder ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder ready.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624365625757
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/cpet_cardiac_training.py\r\n",
        "# Import libraries\r\n",
        "import argparse\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "from azureml.core import Run\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "print(\"Begin run...\")\r\n",
        "# Set regularization parameter\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "#parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\r\n",
        "parser.add_argument('--n-estimators', type=int, dest='n_estimators', default=10, help='n_estimators')\r\n",
        "parser.add_argument(\"--input-data\", type=str, dest='input_data', help='training dataset')\r\n",
        "args = parser.parse_args()\r\n",
        "n_trees = args.n_estimators\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "run.log('Begin run...')\r\n",
        "\r\n",
        "# load the diabetes dataset\r\n",
        "print(\"Loading Data...\")\r\n",
        "run.log('Loading Data...')\r\n",
        "cpet_data = run.input_datasets['training_data'].to_pandas_dataframe() # Get the training data from the estimator input\r\n",
        "run.log('data',cpet_data.shape)\r\n",
        "# {\r\n",
        "#     \"class_name\": \"StandardScaler\",\r\n",
        "#     \"module\": \"sklearn.preprocessing\",\r\n",
        "#     \"param_args\": [],\r\n",
        "#     \"param_kwargs\": {\r\n",
        "#         \"with_mean\": true,\r\n",
        "#         \"with_std\": true\r\n",
        "#     },\r\n",
        "#     \"prepared_kwargs\": {},\r\n",
        "#     \"spec_class\": \"preproc\"\r\n",
        "# }\r\n",
        "\r\n",
        "# {\r\n",
        "#     \"class_name\": \"RandomForestClassifier\",\r\n",
        "#     \"module\": \"sklearn.ensemble\",\r\n",
        "#     \"param_args\": [],\r\n",
        "#     \"param_kwargs\": {\r\n",
        "#         \"bootstrap\": true,\r\n",
        "#         \"class_weight\": \"balanced\",\r\n",
        "#         \"criterion\": \"gini\",\r\n",
        "#         \"max_features\": 0.7,\r\n",
        "#         \"min_samples_leaf\": 0.01,\r\n",
        "#         \"min_samples_split\": 0.15052631578947367,\r\n",
        "#         \"n_estimators\": 10,\r\n",
        "#         \"oob_score\": true\r\n",
        "#     },\r\n",
        "#     \"prepared_kwargs\": {},\r\n",
        "#     \"spec_class\": \"sklearn\"\r\n",
        "# }\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "X, y = cpet_data[['75_to_100_VCO2Slope','VEvsVCO2Slope','75_to_100_VO2Slope','15_to_85_VESlope','MeanVE/VCO2','MeanO2Pulse',\r\n",
        "'PredictedMaxHR','MaxVO2_EST','DiffPeakHR','75_to_100_VEVCO2Slope','StdVE/VCO2','15_to_85_VEVCO2Slope',\r\n",
        "'75_to_100_RRSlope','75_to_100_RERSlope','MeanRR','75_to_100_O2Slope','75_to_100_HRSlope']].values, cpet_data['CardiacLim'].values\r\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\r\n",
        "run.log('X',X)\r\n",
        "\r\n",
        "scaler.fit(X)\r\n",
        "X_scaled = scaler.transform(X)\r\n",
        "\r\n",
        "# Split data into training set and test set\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.30, random_state=0)\r\n",
        "run.log('Model creation')\r\n",
        "\r\n",
        "clf = RandomForestClassifier(bootstrap=True, class_weight=\"balanced\", criterion=\"gini\",max_features=\".7\",min_samples_leaf=0.01,\r\n",
        "                            min_samples_split=0.15052631578947367,n_estimators=n_trees,oob_score=True)\r\n",
        "run.log('Number of trees: ',  np.float(n_trees))\r\n",
        "model = clf.fit(X_train,y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))\r\n",
        "\r\n",
        "# calculate AUC\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\r\n",
        "print('AUC: ' + str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\r\n",
        "joblib.dump(value=model, filename='outputs/cpet_model.pkl')\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cpet_training-hyperdrive/cpet_cardiac_training.py\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Run\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "cpet_ds = ws.datasets.get(\"cpet dataset cardiac2\")\r\n",
        "data_test = cpet_ds.to_pandas_dataframe()\r\n",
        "\r\n",
        "# X, y = data_test[['75_to_100_VCO2Slope','VEvsVCO2Slope','75_to_100_VO2Slope','15_to_85_VESlope','MeanVE/VCO2','MeanO2Pulse',\r\n",
        "# 'PredictedMaxHR','MaxVO2_EST','DiffPeakHR','75_to_100_VEVCO2Slope','StdVE/VCO2','15_to_85_VEVCO2Slope',\r\n",
        "# '75_to_100_RRSlope','75_to_100_RERSlope','MeanRR','75_to_100_O2Slope','75_to_100_HRSlope']].values, data_test['CardiacLim'].values\r\n",
        "\r\n",
        "X, y = data_test[['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','StdVE/VCO2','15_to_85_RRSlope','PeakVE'\r\n",
        " ,'MeanHeartRate','75_to_100_RERSlope','MeanRER','HRvsVO2Slope','75_to_100_VEVCO2Slope','PeakVCO2'\r\n",
        " ,'PredictedMaxHR','75_to_100_VCO2Slope','MeanVE','75_to_100_VESlope','PeakVO2Real'\t\t\r\n",
        " ,'LowestVE/VCO2','VO2atVT','PeakHeartRate']].values, data_test['CardiacLim'].values\r\n",
        "\r\n",
        "#X, y = data_test[['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','StdVE/VCO2','15_to_85_RRSlope','PeakVE'\r\n",
        "#,'MeanHeartRate','75_to_100_RERSlope','MeanRER','HRvsVO2Slope','75_to_100_VEVCO2Slope','PeakVCO2'\r\n",
        "# ,'PredictedMaxHR','75_to_100_VCO2Slope']].values, data_test['CardiacLim'].values\r\n",
        "\r\n",
        "\r\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\r\n",
        "scaler.fit(X)\r\n",
        "X_scaled = scaler.transform(X)\r\n",
        "#print('X',X_scaled)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.70,random_state=123)\r\n",
        "clf = RandomForestClassifier(bootstrap=True, class_weight=\"balanced\", criterion=\"gini\",max_features=0.7,min_samples_leaf=0.01,\r\n",
        "                            min_samples_split=0.15052631578947367,n_estimators=100,oob_score=True, random_state=0)\r\n",
        "#print(clf)\r\n",
        "model = clf.fit(X_train,y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7922077922077922\n"
          ]
        }
      ],
      "execution_count": 118,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624384234730
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import average_precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "y_pred = model.predict_proba(X_test)[:,1]\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\r\n",
        "print(metrics.auc(fpr, tpr))\r\n",
        "average_precision = precision_score(y_test, model.predict(X_test), average='macro')\r\n",
        "print(average_precision)\r\n",
        "macro_recall=recall_score(y_test,  model.predict(X_test), average='macro')\r\n",
        "print(macro_recall)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8456140350877193\n",
            "0.7319892473118279\n",
            "0.6892543859649123\n"
          ]
        }
      ],
      "execution_count": 119,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624384237652
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bechmark_value_2={'Title':'Boruta Features','Columns':['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','75_to_100_VCO2Slope','75_to_100_VESlope','75_to_100_RRSlope'\r\n",
        ",'75_to_100_O2Slope','MeanVO2','StdO2Pulse','75_to_100_RERSlope','HRvsVO2Slope','LowestVO2','PeakVO2'\r\n",
        ",'MeanHeartRate','PeakVO2Real','MeanO2Pulse','PeakRER']}\r\n",
        "bechmark_value_1={'Title':'Grand collection Features','Columns':['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','StdVE/VCO2','15_to_85_RRSlope','PeakVE'\r\n",
        " ,'MeanHeartRate','75_to_100_RERSlope','MeanRER','HRvsVO2Slope','75_to_100_VEVCO2Slope','PeakVCO2'\r\n",
        " ,'PredictedMaxHR','75_to_100_VCO2Slope','MeanVE','75_to_100_VESlope','PeakVO2Real'\t\t\r\n",
        " ,'LowestVE/VCO2','VO2atVT','PeakHeartRate']}\r\n",
        "bechmark_values =[bechmark_value_1, bechmark_value_2] \r\n",
        "for columns in bechmark_values:\r\n",
        "    print('Perofrmance for the',columns['Title'])\r\n",
        "    auc_arr = np.array([])\r\n",
        "    acc_arr = np.array([])\r\n",
        "    pre_arr = np.array([])\r\n",
        "    rec_arr = np.array([])\r\n",
        "    for i in range(35):\r\n",
        "        X, y = data_test[columns['Columns']].values, data_test['CardiacLim'].values\r\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\r\n",
        "        scaler.fit(X)\r\n",
        "        X_scaled = scaler.transform(X)\r\n",
        "        #print('X',X_scaled)\r\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.70,random_state=i)\r\n",
        "        clf = RandomForestClassifier(bootstrap=True, class_weight=\"balanced\", criterion=\"gini\",max_features=0.7,min_samples_leaf=0.01,\r\n",
        "                                    min_samples_split=0.15052631578947367,n_estimators=100,oob_score=True, random_state=i)\r\n",
        "        #print(clf)\r\n",
        "        model = clf.fit(X_train,y_train)\r\n",
        "\r\n",
        "        # calculate accuracy\r\n",
        "        y_hat = model.predict(X_test)\r\n",
        "        acc = np.average(y_hat == y_test)\r\n",
        "        acc_arr = np.append(acc_arr,acc)\r\n",
        "        #print('Accuracy:', acc)\r\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\r\n",
        "        #print(metrics.auc(fpr, tpr))\r\n",
        "        auc_arr = np.append(auc_arr,metrics.auc(fpr, tpr))\r\n",
        "        average_precision = precision_score(y_test, model.predict(X_test), average='macro')\r\n",
        "        pre_arr = np.append(pre_arr, average_precision)\r\n",
        "        #print(average_precision)\r\n",
        "        macro_recall=recall_score(y_test,  model.predict(X_test), average='macro')\r\n",
        "        rec_arr = np.append(rec_arr,macro_recall)\r\n",
        "        #print(macro_recall)\r\n",
        "        pass\r\n",
        "    print('AUC: mean {:.2f}, std {:.2f}'.format(np.mean(auc_arr),np.std(auc_arr)))\r\n",
        "    print('Accuracy: mean {:.2f}, std {:.2f}'.format(np.mean(acc_arr),np.std(acc_arr)))\r\n",
        "    print('Precision: mean {:.2f}, std {:.2f}'.format(np.mean(pre_arr),np.std(pre_arr)))\r\n",
        "    print('Recall: mean {:.2f}, std {:.2f}'.format(np.mean(rec_arr),np.std(rec_arr)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perofrmance for the Grand collection Features\n",
            "AUC: mean 0.81, std 0.03\n",
            "Accuracy: mean 0.75, std 0.03\n",
            "Precision: mean 0.70, std 0.05\n",
            "Recall: mean 0.66, std 0.05\n",
            "Perofrmance for the Boruta Features\n",
            "AUC: mean 0.78, std 0.03\n",
            "Accuracy: mean 0.72, std 0.03\n",
            "Precision: mean 0.65, std 0.04\n",
            "Recall: mean 0.63, std 0.05\n"
          ]
        }
      ],
      "execution_count": 135,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624387473284
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data_test[['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','75_to_100_VCO2Slope','75_to_100_VESlope','75_to_100_RRSlope'\r\n",
        ",'75_to_100_O2Slope','MeanVO2','StdO2Pulse','75_to_100_RERSlope','HRvsVO2Slope','LowestVO2','PeakVO2'\r\n",
        ",'MeanHeartRate','PeakVO2Real','MeanO2Pulse','PeakRER']].values, data_test['CardiacLim'].values\r\n",
        "\r\n",
        "#X, y = data_test[['75_to_100_VO2Slope','DiffPeakVO2','75_to_100_HRSlope','StdVE/VCO2','15_to_85_RRSlope','PeakVE'\r\n",
        "#,'MeanHeartRate','75_to_100_RERSlope','MeanRER','HRvsVO2Slope','75_to_100_VEVCO2Slope','PeakVCO2'\r\n",
        "# ,'PredictedMaxHR','75_to_100_VCO2Slope']].values, data_test['CardiacLim'].values\r\n",
        "\r\n",
        "\r\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\r\n",
        "scaler.fit(X)\r\n",
        "X_scaled = scaler.transform(X)\r\n",
        "#print('X',X_scaled)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.70,random_state=123)\r\n",
        "clf = RandomForestClassifier(bootstrap=True, class_weight=\"balanced\", criterion=\"gini\",max_features=0.7,min_samples_leaf=0.01,\r\n",
        "                            min_samples_split=0.15052631578947367,n_estimators=100,oob_score=True, random_state=0)\r\n",
        "#print(clf)\r\n",
        "model = clf.fit(X_train,y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7792207792207793\n"
          ]
        }
      ],
      "execution_count": 120,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624384240327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import average_precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "y_pred = model.predict_proba(X_test)[:,1]\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\r\n",
        "print(metrics.auc(fpr, tpr))\r\n",
        "average_precision = precision_score(y_test, model.predict(X_test), average='macro')\r\n",
        "print(average_precision)\r\n",
        "macro_recall=recall_score(y_test,  model.predict(X_test), average='macro')\r\n",
        "print(macro_recall)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8208333333333334\n",
            "0.7112903225806452\n",
            "0.6723684210526316\n"
          ]
        }
      ],
      "execution_count": 121,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624384242046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import average_precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "y_pred = model.predict_proba(X_test)[:,1]\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "#print(model.predict_proba(X_test)[:,0])\r\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\r\n",
        "print(metrics.auc(fpr, tpr))\r\n",
        "average_precision = average_precision_score(y_test,y_pred)\r\n",
        "print(average_precision)\r\n",
        "macro_recall=recall_score(y_test,  model.predict(X_test), average='macro')\r\n",
        "print(macro_recall)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.800203873598369\n",
            "0.543929399765017\n",
            "0.6756371049949031\n"
          ]
        }
      ],
      "execution_count": 106,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624383778860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.groupby(['CardiacLim']).count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 82,
          "data": {
            "text/plain": "            MaxVO2_EST  PredictedMaxHR  PeakHeartRate  MeanHeartRate  \\\nCardiacLim                                                             \n0                  158             158            158            158   \n1                   61              61             61             61   \n\n            MinHeartRate  StdHeartRate  LowestVE/VCO2  PeakVE/VCO2  \\\nCardiacLim                                                           \n0                    158           158            158          158   \n1                     61            61             61           61   \n\n            MeanVE/VCO2  StdVE/VCO2  ...  15_to_85_VCO2Slope  \\\nCardiacLim                           ...                       \n0                   158         158  ...                 158   \n1                    61          61  ...                  61   \n\n            15_to_85_VESlope  15_to_85_RERSlope  15_to_85_RRSlope  \\\nCardiacLim                                                          \n0                        158                158               158   \n1                         61                 61                61   \n\n            15_to_85_O2Slope  15_to_85_VEVCO2Slope  15_to_85_VEVO2Slope  \\\nCardiacLim                                                                \n0                        158                   158                  158   \n1                         61                    61                   61   \n\n            VO2atVT  VO2vsPeakVO2atVT  HasAnaerobicThresholdMean  \nCardiacLim                                                        \n0               158               158                        158  \n1                61                61                         61  \n\n[2 rows x 56 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MaxVO2_EST</th>\n      <th>PredictedMaxHR</th>\n      <th>PeakHeartRate</th>\n      <th>MeanHeartRate</th>\n      <th>MinHeartRate</th>\n      <th>StdHeartRate</th>\n      <th>LowestVE/VCO2</th>\n      <th>PeakVE/VCO2</th>\n      <th>MeanVE/VCO2</th>\n      <th>StdVE/VCO2</th>\n      <th>...</th>\n      <th>15_to_85_VCO2Slope</th>\n      <th>15_to_85_VESlope</th>\n      <th>15_to_85_RERSlope</th>\n      <th>15_to_85_RRSlope</th>\n      <th>15_to_85_O2Slope</th>\n      <th>15_to_85_VEVCO2Slope</th>\n      <th>15_to_85_VEVO2Slope</th>\n      <th>VO2atVT</th>\n      <th>VO2vsPeakVO2atVT</th>\n      <th>HasAnaerobicThresholdMean</th>\n    </tr>\n    <tr>\n      <th>CardiacLim</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>...</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n      <td>158</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>...</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 56 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 82,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624382518379
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "158/(158+61)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 84,
          "data": {
            "text/plain": "0.7214611872146118"
          },
          "metadata": {}
        }
      ],
      "execution_count": 84,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624382555331
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"jp-compute-fast-two\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        training_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624369937871
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Create a Python environment for the experiment\r\n",
        "sklearn_env = Environment(\"sklearn-env\")\r\n",
        "\r\n",
        "# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\r\n",
        "packages = CondaDependencies.create(pip_packages=['scikit-learn','azureml-defaults','azureml-dataprep[pandas]'])\r\n",
        "sklearn_env.python.conda_dependencies = packages\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "cpet_ds = ws.datasets.get(\"cpet dataset cardiac2\")\r\n",
        "data_test = cpet_ds.to_pandas_dataframe()\r\n",
        "#print(data_test.head)\r\n",
        "#print(cpet_ds)\r\n",
        "# Create a script config\r\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\r\n",
        "                              script='cpet_cardiac_training.py',\r\n",
        "                              arguments = ['--n-estimators', 10, # Regularizaton rate parameter\r\n",
        "                                           '--input-data', cpet_ds.as_named_input('training_data')], # Reference to dataset\r\n",
        "                              environment=sklearn_env,\r\n",
        "                              compute_target = training_cluster)\r\n",
        "\r\n",
        "# Sample a range of parameter values\r\n",
        "params = GridParameterSampling(\r\n",
        "    {\r\n",
        "        # There's only one parameter, so grid sampling will try each value - with multiple parameters it would try every combination\r\n",
        "        '--n-estimators': choice(10,40,70,100)\r\n",
        "    }\r\n",
        ")\r\n",
        "\r\n",
        "# Configure hyperdrive settings\r\n",
        "hyperdrive = HyperDriveConfig(run_config=script_config, \r\n",
        "                          hyperparameter_sampling=params, \r\n",
        "                          policy=None, \r\n",
        "                          primary_metric_name='AUC', \r\n",
        "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
        "                          max_total_runs=6,\r\n",
        "                          max_concurrent_runs=4)\r\n",
        "\r\n",
        "# Run the experiment\r\n",
        "experiment = Experiment(workspace = ws, name = 'cpet_training_hyperdrive')\r\n",
        "run = experiment.submit(config=hyperdrive)\r\n",
        "\r\n",
        "# Show the status in the notebook as the experiment runs\r\n",
        "RunDetails(run).show()\r\n",
        "run.wait_for_completion()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0adb90b91e4552b8f24c449fbbc6f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Canceled\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f?wsid=/subscriptions/8e837d7c-a522-48b4-9a36-a5863c034b9a/resourcegroups/jp-train-eastus-mlres/workspaces/jp-ml-workspace-train&tid=cb72c54e-4a31-4d9e-b14a-1ea36dfac94c\", \"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"run_properties\": {\"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"created_utc\": \"2021-06-22T14:57:33.457572Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"AUC\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"7c7eaa74-9c62-407b-8270-73ff8c63a8a0\"}, \"tags\": {\"_aml_system_max_concurrent_jobs\": \"4\", \"max_concurrent_jobs\": \"4\", \"_aml_system_max_total_jobs\": \"6\", \"max_total_jobs\": \"6\", \"_aml_system_max_duration_minutes\": \"10080\", \"max_duration_minutes\": \"10080\", \"_aml_system_policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"_aml_system_generator_config\": \"{\\\"name\\\": \\\"GRID\\\", \\\"parameter_space\\\": {\\\"--n-estimators\\\": [\\\"choice\\\", [[10, 40, 70, 100]]]}}\", \"generator_config\": \"{\\\"name\\\": \\\"GRID\\\", \\\"parameter_space\\\": {\\\"--n-estimators\\\": [\\\"choice\\\", [[10, 40, 70, 100]]]}}\", \"_aml_system_primary_metric_config\": \"{\\\"name\\\": \\\"AUC\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"AUC\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"_aml_system_platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://eastus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/8e837d7c-a522-48b4-9a36-a5863c034b9a/resourceGroups/jp-train-eastus-mlres/providers/Microsoft.MachineLearningServices/workspaces/jp-ml-workspace-train/experiments/cpet_training_hyperdrive\\\", \\\"SubscriptionId\\\": \\\"8e837d7c-a522-48b4-9a36-a5863c034b9a\\\", \\\"ResourceGroupName\\\": \\\"jp-train-eastus-mlres\\\", \\\"WorkspaceName\\\": \\\"jp-ml-workspace-train\\\", \\\"ExperimentName\\\": \\\"cpet_training_hyperdrive\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"cpet_cardiac_training.py\\\", \\\"arguments\\\": [\\\"--n-estimators\\\", 10, \\\"--input-data\\\", \\\"DatasetConsumptionConfig:training_data\\\"], \\\"target\\\": \\\"jp-compute-fast-two\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": 2592000, \\\"nodeCount\\\": 1, \\\"priority\\\": null, \\\"environment\\\": {\\\"name\\\": \\\"sklearn-env\\\", \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"scikit-learn\\\", \\\"azureml-defaults~=1.30.0\\\", \\\"azureml-dataprep[pandas]\\\"]}], \\\"channels\\\": [\\\"anaconda\\\", \\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": false, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210513.v1\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}, \\\"platform\\\": {\\\"os\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\"}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"docker\\\": {\\\"useDocker\\\": false, \\\"sharedVolumes\\\": true, \\\"arguments\\\": [], \\\"shmSize\\\": \\\"2g\\\"}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1, \\\"nodeCount\\\": 1}, \\\"pytorch\\\": {\\\"communicationBackend\\\": \\\"nccl\\\", \\\"processCount\\\": null, \\\"nodeCount\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {}, \\\"data\\\": {\\\"training_data\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"d957df74-b539-4218-97cf-04cbac9d3c10\\\", \\\"name\\\": \\\"cpet dataset cardiac2\\\", \\\"version\\\": 6}, \\\"dataPath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"direct\\\", \\\"environmentVariableName\\\": \\\"training_data\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}}, \\\"outputData\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": null}, \\\"credentialPassthrough\\\": false, \\\"command\\\": \\\"\\\"}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"7c7eaa74-9c62-407b-8270-73ff8c63a8a0\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"cb72c54e-4a31-4d9e-b14a-1ea36dfac94c\\\", \\\"amlClientRequestId\\\": \\\"94a973e0-8cbe-4ca5-84f7-12f67a01429f\\\", \\\"amlClientSessionId\\\": \\\"bc46a72d-38e1-4f45-b4bb-0c6c3b17df9d\\\", \\\"subscriptionId\\\": \\\"8e837d7c-a522-48b4-9a36-a5863c034b9a\\\", \\\"estimator\\\": \\\"NoneType\\\", \\\"samplingMethod\\\": \\\"GRID\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 6, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://eastus.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/8e837d7c-a522-48b4-9a36-a5863c034b9a/resourceGroups/jp-train-eastus-mlres/providers/Microsoft.MachineLearningServices/workspaces/jp-ml-workspace-train/experiments/cpet_training_hyperdrive\\\", \\\"SubscriptionId\\\": \\\"8e837d7c-a522-48b4-9a36-a5863c034b9a\\\", \\\"ResourceGroupName\\\": \\\"jp-train-eastus-mlres\\\", \\\"WorkspaceName\\\": \\\"jp-ml-workspace-train\\\", \\\"ExperimentName\\\": \\\"cpet_training_hyperdrive\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"cpet_cardiac_training.py\\\", \\\"arguments\\\": [\\\"--n-estimators\\\", 10, \\\"--input-data\\\", \\\"DatasetConsumptionConfig:training_data\\\"], \\\"target\\\": \\\"jp-compute-fast-two\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": 2592000, \\\"nodeCount\\\": 1, \\\"priority\\\": null, \\\"environment\\\": {\\\"name\\\": \\\"sklearn-env\\\", \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"EXAMPLE_ENV_VAR\\\": \\\"EXAMPLE_VALUE\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"scikit-learn\\\", \\\"azureml-defaults~=1.30.0\\\", \\\"azureml-dataprep[pandas]\\\"]}], \\\"channels\\\": [\\\"anaconda\\\", \\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": false, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210513.v1\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null, \\\"registryIdentity\\\": null}, \\\"platform\\\": {\\\"os\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\"}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": true}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"r\\\": null, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"docker\\\": {\\\"useDocker\\\": false, \\\"sharedVolumes\\\": true, \\\"arguments\\\": [], \\\"shmSize\\\": \\\"2g\\\"}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1, \\\"nodeCount\\\": 1}, \\\"pytorch\\\": {\\\"communicationBackend\\\": \\\"nccl\\\", \\\"processCount\\\": null, \\\"nodeCount\\\": 1}, \\\"paralleltask\\\": {\\\"maxRetriesPerWorker\\\": 0, \\\"workerCountPerNode\\\": 1, \\\"terminalExitCodes\\\": null}, \\\"dataReferences\\\": {}, \\\"data\\\": {\\\"training_data\\\": {\\\"dataLocation\\\": {\\\"dataset\\\": {\\\"id\\\": \\\"d957df74-b539-4218-97cf-04cbac9d3c10\\\", \\\"name\\\": \\\"cpet dataset cardiac2\\\", \\\"version\\\": 6}, \\\"dataPath\\\": null}, \\\"createOutputDirectories\\\": false, \\\"mechanism\\\": \\\"direct\\\", \\\"environmentVariableName\\\": \\\"training_data\\\", \\\"pathOnCompute\\\": null, \\\"overwrite\\\": false}}, \\\"outputData\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": null}, \\\"credentialPassthrough\\\": false, \\\"command\\\": \\\"\\\"}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"7c7eaa74-9c62-407b-8270-73ff8c63a8a0\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"[Scrubbed]\\\", \\\"amlClientFunction\\\": \\\"[Scrubbed]\\\", \\\"tenantId\\\": \\\"cb72c54e-4a31-4d9e-b14a-1ea36dfac94c\\\", \\\"amlClientRequestId\\\": \\\"94a973e0-8cbe-4ca5-84f7-12f67a01429f\\\", \\\"amlClientSessionId\\\": \\\"bc46a72d-38e1-4f45-b4bb-0c6c3b17df9d\\\", \\\"subscriptionId\\\": \\\"8e837d7c-a522-48b4-9a36-a5863c034b9a\\\", \\\"estimator\\\": \\\"NoneType\\\", \\\"samplingMethod\\\": \\\"GRID\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 6, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"vmSize\\\": null}}}\", \"_aml_system_resume_child_runs\": \"null\", \"resume_child_runs\": \"null\", \"_aml_system_all_jobs_generated\": \"true\", \"all_jobs_generated\": \"true\", \"_aml_system_cancellation_requested\": \"true\", \"cancellation_requested\": \"true\", \"_aml_system_progress_metadata_evaluation_timestamp\": \"\\\"2021-06-22T14:57:34.182175\\\"\", \"progress_metadata_evaluation_timestamp\": \"\\\"2021-06-22T14:57:34.182175\\\"\", \"_aml_system_progress_metadata_digest\": \"\\\"2bda5360361ea3a858b42ce9a8fa90c1e86ae0f3c3c5fc1cf1aa45adc4b89b99\\\"\", \"progress_metadata_digest\": \"\\\"2bda5360361ea3a858b42ce9a8fa90c1e86ae0f3c3c5fc1cf1aa45adc4b89b99\\\"\", \"_aml_system_progress_metadata_active_timestamp\": \"\\\"2021-06-22T14:57:34.182175\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2021-06-22T14:57:34.182175\\\"\", \"_aml_system_optimizer_state_artifact\": \"null\", \"_aml_system_outdated_optimizer_state_artifacts\": \"\\\"[]\\\"\", \"_aml_system_HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_0\": \"{\\\"--n-estimators\\\": 10}\", \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_0\": \"{\\\"--n-estimators\\\": 10}\", \"_aml_system_HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_1\": \"{\\\"--n-estimators\\\": 40}\", \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_1\": \"{\\\"--n-estimators\\\": 40}\", \"_aml_system_HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_2\": \"{\\\"--n-estimators\\\": 70}\", \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_2\": \"{\\\"--n-estimators\\\": 70}\", \"_aml_system_HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_3\": \"{\\\"--n-estimators\\\": 100}\", \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_3\": \"{\\\"--n-estimators\\\": 100}\"}, \"end_time_utc\": \"2021-06-22T14:59:33.792548Z\", \"status\": \"Canceled\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://jpmlworkspacet8118195379.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=1A2fnZHUkOJp2uWoYHihOBwzun2oq%2BKpvld3OYid0K8%3D&st=2021-06-22T18%3A52%3A32Z&se=2021-06-23T03%3A02%3A32Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:02:00\", \"run_number\": \"36\", \"run_queued_details\": {\"status\": \"Canceled\", \"details\": null}, \"hyper_parameters\": {\"--n-estimators\": [\"choice\", [[10, 40, 70, 100]]]}}, \"child_runs\": [{\"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_1\", \"run_number\": 39, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-06-22T14:58:13.777816Z\", \"end_time\": \"2021-06-22T14:58:42.297536Z\", \"created_time\": \"2021-06-22T14:58:04.798626Z\", \"created_time_dt\": \"2021-06-22T14:58:04.798626Z\", \"duration\": \"0:00:37\", \"hyperdrive_id\": \"28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"arguments\": null, \"param_--n-estimators\": 40}, {\"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_0\", \"run_number\": 38, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-06-22T14:58:12.198632Z\", \"end_time\": \"2021-06-22T14:58:40.444989Z\", \"created_time\": \"2021-06-22T14:58:04.706892Z\", \"created_time_dt\": \"2021-06-22T14:58:04.706892Z\", \"duration\": \"0:00:35\", \"hyperdrive_id\": \"28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"arguments\": null, \"param_--n-estimators\": 10}, {\"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_2\", \"run_number\": 40, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-06-22T14:58:11.666503Z\", \"end_time\": \"2021-06-22T14:58:42.40945Z\", \"created_time\": \"2021-06-22T14:58:05.2552Z\", \"created_time_dt\": \"2021-06-22T14:58:05.2552Z\", \"duration\": \"0:00:37\", \"hyperdrive_id\": \"28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"arguments\": null, \"param_--n-estimators\": 70}, {\"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_3\", \"run_number\": 37, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2021-06-22T14:58:11.513477Z\", \"end_time\": \"2021-06-22T14:58:39.639581Z\", \"created_time\": \"2021-06-22T14:58:04.702316Z\", \"created_time_dt\": \"2021-06-22T14:58:04.702316Z\", \"duration\": \"0:00:34\", \"hyperdrive_id\": \"28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"arguments\": null, \"param_--n-estimators\": 100}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [{\"name\": \"Accuracy\", \"run_id\": \"HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f\", \"categories\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], \"series\": [{\"data\": [0.7121212121212122, 0.696969696969697, 0.7272727272727273, 0.7556818181818182, 0.7045454545454546, 0.6988636363636364, 0.7272727272727273, 0.7207792207792207, 0.6883116883116883, 0.8441558441558441, 0.7012987012987013, 0.7727272727272727, 0.7792207792207793, 0.7857142857142857, 0.8181818181818182, 0.6883116883116883, 0.7402597402597403, 0.7142857142857143, 0.7532467532467533, 0.7402597402597403, 0.7532467532467533, 0.7597402597402597, 0.7727272727272727, 0.6493506493506493, 0.7922077922077922, 0.7792207792207793, 0.7922077922077922, 0.7792207792207793]}]}], \"run_logs\": \"[2021-06-22T14:57:33.828519][API][INFO]Experiment created\\r\\n[2021-06-22T14:57:34.356775][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\\r\\n[2021-06-22T14:57:34.533427][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\\r\\n[2021-06-22T14:58:04.1013653Z][SCHEDULER][INFO]Scheduling job, id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_1'\\r\\n[2021-06-22T14:58:04.1037256Z][SCHEDULER][INFO]Scheduling job, id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_3'\\r\\n[2021-06-22T14:58:04.1025023Z][SCHEDULER][INFO]Scheduling job, id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_2'\\r\\n[2021-06-22T14:58:04.0999192Z][SCHEDULER][INFO]Scheduling job, id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_0'\\r\\n[2021-06-22T14:58:04.8064088Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_3'\\r\\n[2021-06-22T14:58:04.8325879Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_0'\\r\\n[2021-06-22T14:58:04.9058716Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_1'\\r\\n[2021-06-22T14:58:05.3739960Z][SCHEDULER][INFO]Successfully scheduled a job. Id='HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f_2'\\r\\n[2021-06-22T14:59:03.610786][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\\r\\n[2021-06-22T14:59:03.626325][GENERATOR][WARNING]Could not sample any more jobs from the space.\\r\\n[2021-06-22T14:59:03.893451][CONTROLLER][INFO]Experiment has been marked for cancellation.\\r\\n[2021-06-22T14:59:03.893516][CONTROLLER][WARNING]The first 3 jobs have failed. The system is canceling the experiment. Please resolve the issues before resubmitting the experiment.\\r\\n[2021-06-22T14:59:33.532020][CONTROLLER][WARNING]User errors were found in at least one of the child runs.\\r\\n[2021-06-22T14:59:34.012774][CONTROLLER][INFO]Experiment was 'ExperimentStatus.RUNNING', is 'ExperimentStatus.CANCELLED'.\\n\\nError occurred: User errors were found in at least one of the child runs.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.30.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "{'runId': 'HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f',\n 'target': 'jp-compute-fast-two',\n 'status': 'Canceled',\n 'startTimeUtc': '2021-06-22T14:57:33.54132Z',\n 'endTimeUtc': '2021-06-22T14:59:33.792548Z',\n 'error': {'error': {'code': 'UserError',\n   'message': 'User errors were found in at least one of the child runs.',\n   'messageParameters': {},\n   'details': []},\n  'time': '0001-01-01T00:00:00.000Z'},\n 'properties': {'primary_metric_config': '{\"name\": \"AUC\", \"goal\": \"maximize\"}',\n  'resume_from': 'null',\n  'runTemplate': 'HyperDrive',\n  'azureml.runsource': 'hyperdrive',\n  'platform': 'AML',\n  'ContentSnapshotId': '7c7eaa74-9c62-407b-8270-73ff8c63a8a0'},\n 'inputDatasets': [],\n 'outputDatasets': [],\n 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://jpmlworkspacet8118195379.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_28c7a600-3954-4cf6-8bef-b5aa8b37b56f/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=AhV3rdr86tW8kVhz5%2BTSMj4OB1Z9Dj%2BPPbRTPMC%2BbIM%3D&st=2021-06-22T14%3A49%3A41Z&se=2021-06-22T22%3A59%3A41Z&sp=r'},\n 'submittedBy': 'Julio Portella'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1624374001313
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}